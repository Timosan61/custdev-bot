from typing import Dict, Optional
from aiogram import types, Bot
from aiogram.fsm.context import FSMContext
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from loguru import logger
import json
import os

from src.services.supabase_service import SupabaseService
from src.services.zep_service import ZepService
from src.services.voice_handler import VoiceMessageHandler
from src.utils.keyboards import get_finish_keyboard
from src.state.user_states import ResearcherStates

class ResearcherAgent:
    def __init__(self, supabase: SupabaseService, zep: ZepService):
        self.supabase = supabase
        self.zep = zep
        # Initialize voice handler with bot token
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
        self.voice_handler = VoiceMessageHandler(bot_token=bot_token)
        self.llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7)
        
        self.fields_to_collect = {
            "research_goal": "–¶–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
            "audience": "–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è",
            "hypotheses": "–ì–∏–ø–æ—Ç–µ–∑—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏",
            "style": "–°—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è",
            "topic": "–¢–µ–º–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç"
        }
    
    async def start_dialog(self, message: types.Message, state: FSMContext):
        user_id = message.from_user.id
        
        # Create new interview
        interview = self.supabase.create_interview({})
        if not interview:
            await message.answer("‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä–≤—å—é")
            return
        interview_id = interview["id"]
        
        # Create Zep session
        zep_session_id = f"researcher_{user_id}_{interview_id}"
        await self.zep.create_session(zep_session_id, {
            "user_id": user_id,
            "interview_id": interview_id,
            "type": "researcher"
        })
        
        # Save to state
        await state.update_data(
            interview_id=interview_id,
            zep_session_id=zep_session_id,
            collected_fields={}
        )
        
        # Send welcome message and first question
        welcome_text = (
            "üî¨ <b>–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è</b>\n\n"
            "–ü—Ä–∏–≤–µ—Ç! –Ø –ø–æ–º–æ–≥—É –≤–∞–º —Å–æ–∑–¥–∞—Ç—å –∫–∞—Å—Ç–¥–µ–≤-–∏–Ω—Ç–µ—Ä–≤—å—é.\n\n"
            "–û—Ç–≤–µ—á–∞–π—Ç–µ –Ω–∞ –º–æ–∏ –≤–æ–ø—Ä–æ—Å—ã –≤ —Å–≤–æ–±–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–µ —Ç–µ–∫—Å—Ç–æ–º –∏–ª–∏ –≥–æ–ª–æ—Å–æ–º. "
            "–ö–æ–≥–¥–∞ –∑–∞–∫–æ–Ω—á–∏—Ç–µ, —Å–∫–∞–∂–∏—Ç–µ '—Ö–≤–∞—Ç–∏—Ç' –∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è."
        )
        
        await message.answer(welcome_text, reply_markup=get_finish_keyboard())
        
        # Ask first question
        first_question = "–ö–∞–∫–æ–≤–∞ –æ—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å –≤–∞—à–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è? –ß—Ç–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ —É–∑–Ω–∞—Ç—å –∏–ª–∏ –ø–æ–Ω—è—Ç—å?"
        await message.answer(first_question)
        
        # Save first question in state
        await state.update_data(last_question=first_question)
        
        # Log to Zep
        await self.zep.add_message(zep_session_id, "assistant", welcome_text)
        await self.zep.add_message(zep_session_id, "assistant", first_question)
    
    async def process_text_message(self, message: types.Message, state: FSMContext):
        await self._process_message(message.text, message, state)
    
    async def process_voice_message(self, message: types.Message, state: FSMContext, bot: Bot):
        # Send processing indicator
        processing_msg = await message.answer("üé§ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ...")
        
        # Process voice message
        result = await self.voice_handler.process_voice_message(
            file_id=message.voice.file_id,
            duration=message.voice.duration
        )
        
        # Delete processing message
        await bot.delete_message(message.chat.id, processing_msg.message_id)
        
        if result["success"]:
            text = result["transcription"]
            logger.info(f"Voice transcribed: {text}")
            
            # Show what was recognized
            await message.answer(f"‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: <i>{text}</i>")
            
            # Process the message
            await self._process_message(text, message, state)
        else:
            error = result.get("error", "Unknown error")
            logger.error(f"Voice processing failed: {error}")
            await message.answer("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç–æ–º.")
    
    async def _process_message(self, text: str, message: types.Message, state: FSMContext):
        data = await state.get_data()
        user_id = message.from_user.id
        interview_id = data.get("interview_id")
        zep_session_id = data.get("zep_session_id")
        collected_fields = data.get("collected_fields", {})
        last_question = data.get("last_question", "")
        
        logger.info(f"Processing message from user {user_id}: {text[:50]}...")
        logger.debug(f"Current collected fields: {collected_fields}")
        logger.debug(f"Last question was: {last_question}")
        
        # Log user message to Zep
        await self.zep.add_message(zep_session_id, "user", text)
        
        # Check if user wants to finish
        if any(word in text.lower() for word in ["—Ö–≤–∞—Ç–∏—Ç", "–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ", "–≤—Å–µ", "‚úÖ –∑–∞–≤–µ—Ä—à–∏—Ç—å"]):
            await self._finish_collection(message, state)
            return
        
        # Analyze message and extract fields
        extracted = await self._analyze_message(text, collected_fields, last_question)
        logger.info(f"Extracted fields: {extracted}")
        
        if extracted:
            collected_fields.update(extracted)
            logger.info(f"Updated collected fields: {collected_fields}")
        
        # Update state with new fields and last question
        await state.update_data(collected_fields=collected_fields)
        
        # Generate next question with context of last answer
        next_question = await self._generate_next_question(collected_fields, text)
        
        if next_question:
            await message.answer(next_question)
            await self.zep.add_message(zep_session_id, "assistant", next_question)
            # Save the question for context
            await state.update_data(last_question=next_question)
        else:
            await self._finish_collection(message, state)
    
    async def _analyze_message(self, text: str, current_fields: Dict, last_question: str = "") -> Dict:
        prompt = PromptTemplate(
            input_variables=["text", "current_fields", "all_fields", "last_question"],
            template="""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –≤–æ–ø—Ä–æ—Å –∏ –∏–∑–≤–ª–µ–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø–æ–ª–µ–π –∫–∞—Å—Ç–¥–µ–≤-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.
            
            –ü–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–¥–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å:
            {last_question}
            
            –û—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
            {text}
            
            –£–∂–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –ø–æ–ª—è:
            {current_fields}
            
            –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è –∏ –∏—Ö –æ–ø–∏—Å–∞–Ω–∏—è:
            {all_fields}
            
            –í–ê–ñ–ù–û: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –Ω–µ—Ç–æ—á–Ω–æ –∏–ª–∏ –∫—Ä–∞—Ç–∫–æ. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–Ω—è—Ç—å —Å–º—ã—Å–ª –æ—Ç–≤–µ—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.
            
            –ü—Ä–∏–º–µ—Ä—ã —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è:
            - –í–æ–ø—Ä–æ—Å –æ —Ü–µ–ª–∏ -> "—É–∑–Ω–∞—Ç—å –±–æ–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤" = research_goal: "–í—ã—è–≤–ª–µ–Ω–∏–µ –±–æ–ª–µ–π –∏ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π –∫–ª–∏–µ–Ω—Ç–æ–≤"
            - –í–æ–ø—Ä–æ—Å –æ —Ü–µ–ª–∏ -> "–∞—É–¥–∏—Ç–æ—Ä–∏—è –º–æ–∏—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤" = research_goal: "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏"
            - –í–æ–ø—Ä–æ—Å –æ–± –∞—É–¥–∏—Ç–æ—Ä–∏–∏ -> "–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–∏" = audience: "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–∏"
            
            –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏ (–±–µ–∑ markdown —Ä–∞–∑–º–µ—Ç–∫–∏). 
            –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø–æ–ª—è, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π –æ–±—ä–µ–∫—Ç {{}}.
            
            –ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {{"research_goal": "–ò–∑—É—á–∏—Ç—å –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤"}}
            """
        )
        
        try:
            response = await self.llm.ainvoke(
                prompt.format(
                    text=text,
                    current_fields=json.dumps(current_fields, ensure_ascii=False),
                    all_fields=json.dumps(self.fields_to_collect, ensure_ascii=False),
                    last_question=last_question
                )
            )
            
            logger.debug(f"LLM response: {response.content}")
            
            # Extract JSON from markdown code blocks if present
            content = response.content.strip()
            if content.startswith("```json") and content.endswith("```"):
                content = content[7:-3].strip()
            elif content.startswith("```") and content.endswith("```"):
                content = content[3:-3].strip()
            
            result = json.loads(content)
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from LLM: {e}")
            logger.error(f"LLM response was: {response.content if 'response' in locals() else 'No response'}")
            return {}
        except Exception as e:
            logger.error(f"Error in analyze_message: {e}")
            return {}
    
    async def _generate_next_question(self, collected_fields: Dict, last_answer: str = "") -> Optional[str]:
        missing_fields = [
            field for field in self.fields_to_collect 
            if field not in collected_fields or not collected_fields[field]
        ]
        
        if not missing_fields:
            return None
        
        prompt = PromptTemplate(
            input_variables=["collected", "missing", "descriptions", "last_answer"],
            template="""
            –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å –¥–ª—è —Å–±–æ—Ä–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
            
            –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
            {last_answer}
            
            –£–∂–µ —Å–æ–±—Ä–∞–Ω–æ:
            {collected}
            
            –ù–µ–¥–æ—Å—Ç–∞–µ—Ç:
            {missing}
            
            –û–ø–∏—Å–∞–Ω–∏—è –ø–æ–ª–µ–π:
            {descriptions}
            
            –í–ê–ñ–ù–û: –ù–∞—á–Ω–∏ –æ—Ç–≤–µ—Ç —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Ç–æ–≥–æ, —á—Ç–æ —Ç—ã –ø–æ–Ω—è–ª –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            –ó–∞—Ç–µ–º –∑–∞–¥–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ –û–î–ù–û–ú –∏–∑ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–æ–ª–µ–π.
            
            –ü—Ä–∏–º–µ—Ä—ã —Ö–æ—Ä–æ—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤:
            - "–ü–æ–Ω—è–ª, –≤—ã —Ö–æ—Ç–∏—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –º–∞–º–æ—á–µ–∫ 30-40 –ª–µ—Ç. –ö–∞–∫–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –∞—Å–ø–µ–∫—Ç—ã –∏—Ö –ø–æ–≤–µ–¥–µ–Ω–∏—è –∏–ª–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç?"
            - "–û—Ç–ª–∏—á–Ω–æ, –≤–∞—à–∞ —Ü–µ–ª—å - –≤—ã—è–≤–∏—Ç—å –±–æ–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ —Å—Ñ–µ—Ä–µ –¥–µ—Ç—Å–∫–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤. –ö–∞–∫–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≤ —Ö–æ–¥–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è?"
            - "–•–æ—Ä–æ—à–æ, —è –ø–æ–Ω—è–ª —á—Ç–æ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ —Å –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—è–º–∏. –í –∫–∞–∫–æ–º —Å—Ç–∏–ª–µ –¥–æ–ª–∂–µ–Ω –æ–±—â–∞—Ç—å—Å—è –±–æ—Ç —Å —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–∞–º–∏ - –±–æ–ª–µ–µ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–º –∏–ª–∏ –¥—Ä—É–∂–µ—Å–∫–æ–º?"
            
            –ë—É–¥—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏ –ø–æ–∫–∞–∂–∏, —á—Ç–æ —Ç—ã –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —Å–ª—É—à–∞–µ—à—å —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞.
            """
        )
        
        response = await self.llm.ainvoke(
            prompt.format(
                last_answer=last_answer,
                collected=collected_fields,
                missing=missing_fields,
                descriptions=self.fields_to_collect
            )
        )
        
        return response.content
    
    async def _finish_collection(self, message: types.Message, state: FSMContext):
        data = await state.get_data()
        interview_id = data.get("interview_id")
        collected_fields = data.get("collected_fields", {})
        
        # Generate instruction
        instruction = await self._generate_instruction(collected_fields)
        
        # Update interview
        update_data = {
            "status": "in_progress",
            "fields": collected_fields
        }
        
        # –ü–æ–∫–∞ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ instruction, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ—ë –≤ fields
        if instruction:
            update_data["fields"]["instruction"] = instruction
            
        self.supabase.update_interview(interview_id, update_data)
        
        # Generate interview link
        bot_username = (await message.bot.me()).username
        interview_link = f"https://t.me/{bot_username}?start=interview_{interview_id}"
        
        # Send result
        result_text = (
            "‚úÖ <b>–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ!</b>\n\n"
            f"<b>–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –∏–Ω—Ç–µ—Ä–≤—å—é:</b>\n{instruction}\n\n"
            f"<b>–°—Å—ã–ª–∫–∞ –¥–ª—è —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–æ–≤:</b>\n"
            f"<code>{interview_link}</code>\n\n"
            "–û—Ç–ø—Ä–∞–≤—å—Ç–µ —ç—Ç—É —Å—Å—ã–ª–∫—É –≤–∞—à–∏–º —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–∞–º –¥–ª—è –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–≤—å—é."
        )
        
        await message.answer(result_text, reply_markup=types.ReplyKeyboardRemove())
        await state.clear()
    
    async def _generate_instruction(self, fields: Dict) -> str:
        with open("src/prompts/instruction_generator.txt", "r") as f:
            template = f.read()
        
        prompt = PromptTemplate(
            input_variables=["fields"],
            template=template
        )
        
        response = await self.llm.ainvoke(prompt.format(fields=fields))
        return response.content